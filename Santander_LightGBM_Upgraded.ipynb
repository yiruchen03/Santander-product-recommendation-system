{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a4515b",
   "metadata": {},
   "source": [
    "# Santander LightGBM Upgraded Pipeline\n",
    "_Last generated: 2025-10-06T16:21:35_\n",
    "\n",
    "This notebook provides a **production-leaning** upgrade of your Santander hybrid recommendation system:\n",
    "- Robust **feature engineering** for tabular data\n",
    "- **LightGBM** with 5-fold CV and early stopping\n",
    "- **Hyperparameter tuning** (optional, Optuna)\n",
    "- **Explainability** via SHAP\n",
    "- **Fairness auditing** by group\n",
    "- Save **versioned artifacts** (model + encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a0740",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q pandas numpy scikit-learn lightgbm shap optuna pyarrow\n",
    "import os, json, gc, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "BASE_DIR = Path(\".\")\n",
    "ARTIFACT_DIR = Path(\"models\"); ARTIFACT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "REPORT_DIR = Path(\"reports\"); REPORT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "DATA_DIR = Path(\"data/processed\"); DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eccd9ab",
   "metadata": {},
   "source": [
    "## 2) Data Loading\n",
    "Fill the file paths and label/feature settings below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TODO: set your processed dataset paths ====\n",
    "# Expected: tabular classification with binary label\n",
    "TRAIN_PATH = \"data/processed/train.parquet\"   # or .csv\n",
    "VALID_PATH = \"data/processed/valid.parquet\"   # optional; else split from train\n",
    "TEST_PATH  = \"data/processed/test.parquet\"    # optional\n",
    "\n",
    "# ==== TODO: set your column names ====\n",
    "LABEL_COL = \"target\"     # binary label column\n",
    "ID_COL    = \"customer_id\"  # optional unique id\n",
    "GROUP_COL = None         # e.g., \"region\" or \"gender\" for fairness auditing; set None if unavailable\n",
    "\n",
    "# Load data (change to read_csv if needed)\n",
    "def safe_read(path):\n",
    "    if str(path).endswith(\".parquet\"):\n",
    "        return pd.read_parquet(path)\n",
    "    elif str(path).endswith(\".csv\"):\n",
    "        return pd.read_csv(path)\n",
    "    else:\n",
    "        raise ValueError(\"Use .parquet or .csv\")\n",
    "\n",
    "df = safe_read(TRAIN_PATH) if os.path.exists(TRAIN_PATH) else None\n",
    "df_valid = safe_read(VALID_PATH) if os.path.exists(VALID_PATH) else None\n",
    "df_test = safe_read(TEST_PATH) if os.path.exists(TEST_PATH) else None\n",
    "\n",
    "if df is None:\n",
    "    print(\"⚠️ Could not find training data at\", TRAIN_PATH, \"\\nPlease place your processed training data file.\")\n",
    "else:\n",
    "    print(\"✅ Train shape:\", df.shape)\n",
    "    if df_valid is not None: print(\"✅ Valid shape:\", df_valid.shape)\n",
    "    if df_test is not None:  print(\"✅ Test  shape:\", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c33f11",
   "metadata": {},
   "source": [
    "## 3) Feature Engineering (Minimal Template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb2dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_label(df, label_col, id_col=None):\n",
    "    cols = [c for c in df.columns if c != label_col and c != id_col]\n",
    "    X = df[cols].copy()\n",
    "    y = df[label_col].astype(int).values if label_col in df.columns else None\n",
    "    return X, y, cols\n",
    "\n",
    "def detect_categorical(df: pd.DataFrame, max_cardinality: int = 64) -> List[str]:\n",
    "    cats = []\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_object_dtype(df[c]) or pd.api.types.is_categorical_dtype(df[c]):\n",
    "            cats.append(c)\n",
    "        elif pd.api.types.is_integer_dtype(df[c]) and df[c].nunique() <= max_cardinality:\n",
    "            cats.append(c)\n",
    "    return cats\n",
    "\n",
    "def prepare_data(df, label_col, id_col=None):\n",
    "    X, y, cols = split_features_label(df, label_col, id_col)\n",
    "    cat_cols = detect_categorical(X)\n",
    "    enc = None\n",
    "    if len(cat_cols) > 0:\n",
    "        enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        X[cat_cols] = enc.fit_transform(X[cat_cols].astype(str))\n",
    "    return X, y, cols, cat_cols, enc\n",
    "\n",
    "if df is not None:\n",
    "    X, y, features, cat_cols, enc = prepare_data(df, LABEL_COL, ID_COL)\n",
    "    print(\"Features:\", len(features), \"| Categorical:\", len(cat_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b115c36",
   "metadata": {},
   "source": [
    "## 4) LightGBM with 5-Fold CV + Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm_cv(X, y, cat_cols: List[str], seed=SEED, n_splits=5):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof_pred = np.zeros(len(X))\n",
    "    models = []\n",
    "    aucs, aps = [], []\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 64,\n",
    "        \"max_depth\": -1,\n",
    "        \"min_data_in_leaf\": 50,\n",
    "        \"feature_fraction\": 0.9,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"lambda_l1\": 0.0,\n",
    "        \"lambda_l2\": 0.0,\n",
    "        \"verbosity\": -1,\n",
    "        \"seed\": seed\n",
    "    }\n",
    "\n",
    "    for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_trn, y_trn = X.iloc[trn_idx], y[trn_idx]\n",
    "        X_val, y_val = X.iloc[val_idx], y[val_idx]\n",
    "\n",
    "        lgb_trn = lgb.Dataset(X_trn, label=y_trn, categorical_feature=cat_cols, free_raw_data=False)\n",
    "        lgb_val = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_cols, free_raw_data=False)\n",
    "\n",
    "        model = lgb.train(params,\n",
    "                          lgb_trn,\n",
    "                          num_boost_round=5000,\n",
    "                          valid_sets=[lgb_trn, lgb_val],\n",
    "                          valid_names=[\"train\", \"valid\"],\n",
    "                          early_stopping_rounds=100,\n",
    "                          verbose_eval=200)\n",
    "\n",
    "        pred_val = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        oof_pred[val_idx] = pred_val\n",
    "        auc = roc_auc_score(y_val, pred_val)\n",
    "        ap  = average_precision_score(y_val, pred_val)\n",
    "        aucs.append(auc); aps.append(ap)\n",
    "        models.append(model)\n",
    "        print(f\"[Fold {fold}] AUC={auc:.4f} | AP={ap:.4f} | Best iters={model.best_iteration}\")\n",
    "\n",
    "    print(\"OOF AUC:\", roc_auc_score(y, oof_pred))\n",
    "    print(\"OOF AP :\", average_precision_score(y, oof_pred))\n",
    "    return models, oof_pred, {\"auc_per_fold\": aucs, \"ap_per_fold\": aps, \"oof_auc\": float(roc_auc_score(y, oof_pred)), \"oof_ap\": float(average_precision_score(y, oof_pred))}\n",
    "\n",
    "if df is not None:\n",
    "    models, oof_pred, cv_report = train_lgbm_cv(X, y, cat_cols)\n",
    "    with open(REPORT_DIR / \"metrics.json\", \"w\") as f:\n",
    "        json.dump(cv_report, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b04a171",
   "metadata": {},
   "source": [
    "## 5) Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# pick the best model by validation score observed (here simply take first as placeholder)\n",
    "best_model = models[0] if len(models) > 0 else None\n",
    "if best_model is not None:\n",
    "    best_model.save_model(str(ARTIFACT_DIR / \"lgbm_model.txt\"), num_iteration=best_model.best_iteration)\n",
    "    print(\"✅ Saved:\", ARTIFACT_DIR / \"lgbm_model.txt\")\n",
    "\n",
    "if 'enc' in globals() and enc is not None:\n",
    "    with open(ARTIFACT_DIR / \"encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(enc, f)\n",
    "    print(\"✅ Saved:\", ARTIFACT_DIR / \"encoder.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e5e17",
   "metadata": {},
   "source": [
    "## 6) SHAP Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc1cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if best_model is not None:\n",
    "    # shap for tree models\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    # Use a sample for speed\n",
    "    sample_X = X.sample(min(500, len(X)), random_state=SEED)\n",
    "    shap_values = explainer.shap_values(sample_X)\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, sample_X, show=False)  # don't display in some environments\n",
    "    plt.tight_layout()\n",
    "    out_path = REPORT_DIR / \"shap_summary.png\"\n",
    "    plt.savefig(out_path, dpi=160)\n",
    "    print(\"✅ Saved SHAP summary to\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e0121",
   "metadata": {},
   "source": [
    "## 7) Fairness Audit by Group (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e89371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "def group_metrics(df_in, y_true, y_prob, group_col, thr=0.5, min_n=30):\n",
    "    report = []\n",
    "    for g, idx in df_in.groupby(group_col).groups.items():\n",
    "        idx = list(idx)\n",
    "        if len(idx) < min_n:\n",
    "            continue\n",
    "        auc = roc_auc_score(y_true[idx], y_prob[idx])\n",
    "        f1  = f1_score(y_true[idx], (y_prob[idx] > thr).astype(int))\n",
    "        report.append({\"group\": str(g), \"n\": int(len(idx)), \"auc\": float(auc), \"f1\": float(f1)})\n",
    "    return pd.DataFrame(report).sort_values(\"auc\", ascending=False)\n",
    "\n",
    "if df is not None and GROUP_COL is not None and GROUP_COL in df.columns:\n",
    "    fairness_df = group_metrics(df.reset_index(drop=True), y, oof_pred, GROUP_COL)\n",
    "    fairness_df.to_csv(REPORT_DIR / \"fairness_by_group.csv\", index=False)\n",
    "    print(\"✅ Saved fairness report to\", REPORT_DIR / \"fairness_by_group.csv\")\n",
    "else:\n",
    "    print(\"ℹ️ Set GROUP_COL to a valid column to enable fairness auditing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840d23a",
   "metadata": {},
   "source": [
    "## 8) Batch Inference Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca48a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(model, df_in, enc=None, id_col=None):\n",
    "    dfc = df_in.copy()\n",
    "    if enc is not None:\n",
    "        cat_cols = [c for c in dfc.columns if str(dfc[c].dtype) == \"object\"]\n",
    "        if len(cat_cols) > 0:\n",
    "            dfc[cat_cols] = enc.transform(dfc[cat_cols].astype(str))\n",
    "    Xb = dfc.drop(columns=[c for c in [LABEL_COL, id_col] if c and c in dfc.columns], errors=\"ignore\")\n",
    "    probs = model.predict(Xb, num_iteration=model.best_iteration)\n",
    "    out = pd.DataFrame({\n",
    "        id_col if id_col and id_col in df_in.columns else \"row_id\": df_in[id_col] if id_col and id_col in df_in.columns else np.arange(len(df_in)),\n",
    "        \"prob\": probs\n",
    "    })\n",
    "    return out\n",
    "\n",
    "if best_model is not None and df_valid is not None:\n",
    "    preds = batch_predict(best_model, df_valid, enc=enc, id_col=ID_COL)\n",
    "    preds.head()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
