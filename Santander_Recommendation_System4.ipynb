{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FLfVOGkg7uU"
   },
   "source": [
    "# Santander Product Recommendation System\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### Part 1: Data Preparation\n",
    "* [1. Data Loading](#section-1) - Load and combine training/test datasets with memory optimization\n",
    "* [2. Data Cleaning](#section-2) - Handle missing values and data quality issues\n",
    "* [3. Feature Engineering](#section-3) - Create 193 engineered features for better predictions\n",
    "\n",
    "### Part 2: Model Development\n",
    "* [4. Train/Validation Split](#section-4) - Time-based split strategy for recommendation systems\n",
    "* [5. LightGBM Model Training](#section-5) - Train 24 binary classifiers (one per product)\n",
    "* [6. Model Evaluation](#section-6) - Compute MAP@7 and AUC metrics\n",
    "\n",
    "### Part 3: Model Analysis\n",
    "* [7. Fairness Testing](#section-7) - Evaluate recommendation bias across demographic groups\n",
    "  * 7.1 Fairness by Age\n",
    "  * 7.2 Fairness by Gender\n",
    "  * 7.3 Fairness by Income\n",
    "  * 7.4 Cross-Group Analysis\n",
    "\n",
    "### Part 4: Deployment\n",
    "* [8. Model Export](#section-8) - Save models and artifacts for production deployment\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project Goals\n",
    "\n",
    "Build a recommendation system that predicts which financial products customers are likely to purchase next, helping Santander Bank:\n",
    "- **Personalize** product offerings to individual customers\n",
    "- **Increase** cross-sell conversion rates\n",
    "- **Ensure** fair treatment across demographic groups\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Key Results\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Dataset Size** | 10M+ customer records |\n",
    "| **Time Period** | Oct 2015 - Jun 2016 (9 months) |\n",
    "| **Products** | 24 financial products |\n",
    "| **Models Trained** | 21 LightGBM classifiers |\n",
    "| **MAP@7** (Model only) | 0.0141 |\n",
    "| **MAP@7** (Hybrid) | 0.0147 |\n",
    "| **Average AUC** | ~0.85 |\n",
    "| **Features** | 193 engineered features |\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Tech Stack\n",
    "\n",
    "- **Data Processing**: Pandas, NumPy\n",
    "- **Modeling**: LightGBM, Scikit-learn\n",
    "- **Visualization**: Matplotlib, Seaborn\n",
    "- **Deployment**: FastAPI, Docker\n",
    "\n",
    "---\n",
    "\n",
    "**GitHub Repository**: https://github.com/yiruchen03/Santander-product-recommendation-system\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "FoydDdaeg7uV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from timeit import default_timer as timer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc  \n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZATKczNg7uV"
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "## Get Data\n",
    "The dataset contains purchase data about the bank's customers and the bank's products.\n",
    "The data starts at 2015-01-28 and has monthly records of products a customer has, such as \"credit\n",
    "card\", \"savings account\", etc. To save memory, I only train on data from 2015-06 to 2016-05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lo8cynHYg7uV",
    "outputId": "445f9999-6311-43d4-8498-918804a7c7d7",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8261972, 48)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "train_zip = os.path.expanduser(\"~/Downloads/santander-product-recommendation/train_ver2.csv.zip\")\n",
    "test_zip  = os.path.expanduser(\"~/Downloads/santander-product-recommendation/test_ver2.csv.zip\")\n",
    "\n",
    "train_csv_name = \"train_ver2.csv\"\n",
    "test_csv_name  = \"test_ver2.csv\"\n",
    "\n",
    "need_months = pd.period_range(\"2015-10\", \"2016-06\", freq=\"M\")\n",
    "\n",
    "def read_zip_csv_filtered(zip_path, csv_name, chunksize=1_000_000, usecols=None, dtypes=None):\n",
    "    \"\"\"read from zip file and filter by month\"\"\"\n",
    "    out_chunks = []\n",
    "    with zipfile.ZipFile(zip_path) as zf:\n",
    "        with zf.open(csv_name) as f:\n",
    "            for chunk in pd.read_csv(\n",
    "                f,\n",
    "                chunksize=chunksize,\n",
    "                dtype=dtypes,\n",
    "                usecols=usecols,\n",
    "                low_memory=False\n",
    "            ):\n",
    "                # parse date (only parse the date we need)\n",
    "                if \"fecha_dato\" in chunk.columns:\n",
    "                    chunk[\"fecha_dato\"] = pd.to_datetime(chunk[\"fecha_dato\"], errors=\"coerce\")\n",
    "                    per = chunk[\"fecha_dato\"].dt.to_period(\"M\")\n",
    "                    chunk = chunk[per.isin(need_months)]\n",
    "                # append\n",
    "                out_chunks.append(chunk)\n",
    "    if not out_chunks:\n",
    "        return pd.DataFrame()\n",
    "    return pd.concat(out_chunks, axis=0, ignore_index=True)\n",
    "\n",
    "    # claim some light dtypes to save memory\n",
    "dtypes_hint = {\n",
    "    \"ncodpers\": \"int64\",\n",
    "    \"sexo\": \"object\",\n",
    "    \"age\": \"object\",\n",
    "    \"renta\": \"object\",\n",
    "   \n",
    "}\n",
    "\n",
    "df_train = read_zip_csv_filtered(train_zip, train_csv_name, dtypes=dtypes_hint)\n",
    "df_test  = read_zip_csv_filtered(test_zip,  test_csv_name,  dtypes=dtypes_hint)\n",
    "df_all = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "# map sexo to 0 and 1\n",
    "df_all['sexo'] = df_all['sexo'].map({'H': 0, 'V': 1})\n",
    "\n",
    "# Check the shape of the concatenated DataFrame\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatetimeArray>\n",
       "['2015-10-28 00:00:00', '2015-11-28 00:00:00', '2015-12-28 00:00:00',\n",
       " '2016-01-28 00:00:00', '2016-02-28 00:00:00', '2016-03-28 00:00:00',\n",
       " '2016-04-28 00:00:00', '2016-05-28 00:00:00', '2016-06-28 00:00:00']\n",
       "Length: 9, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[\"fecha_dato\"] = pd.to_datetime(df_all[\"fecha_dato\"],format=\"%Y-%m-%d\")\n",
    "df_all[\"fecha_alta\"] = pd.to_datetime(df_all[\"fecha_alta\"],format=\"%Y-%m-%d\")\n",
    "df_all[\"fecha_dato\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data cleaning\n",
    "\n",
    "I tried some data cleaning methods like filling na with mode/mean but the impact turned out to be very little. So I did not use much of the cleaning methods in this version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already has nomprov as province name, so drop cod_prov and tipodom\n",
    "df_all.drop([\"tipodom\",\"conyuemp\",'cod_prov'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/1380141501.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_all[col].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 1. get all product columns and convert to numeric\n",
    "product_cols = [col for col in df_all.columns if col.startswith('ind_') and 'ult1' in col]\n",
    "for col in product_cols:\n",
    "    df_all[col] = pd.to_numeric(df_all[col], errors='coerce')\n",
    "    df_all[col].fillna(0, inplace=True)\n",
    "    df_all[col] = df_all[col].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               False\n",
       "ncodpers                 False\n",
       "ind_empleado             False\n",
       "pais_residencia          False\n",
       "sexo                      True\n",
       "age                      False\n",
       "fecha_alta               False\n",
       "ind_nuevo                False\n",
       "antiguedad               False\n",
       "indrel                   False\n",
       "ult_fec_cli_1t            True\n",
       "indrel_1mes               True\n",
       "tiprel_1mes               True\n",
       "indresi                  False\n",
       "indext                   False\n",
       "canal_entrada             True\n",
       "indfall                  False\n",
       "nomprov                   True\n",
       "ind_actividad_cliente    False\n",
       "renta                     True\n",
       "segmento                  True\n",
       "ind_ahor_fin_ult1        False\n",
       "ind_aval_fin_ult1        False\n",
       "ind_cco_fin_ult1         False\n",
       "ind_cder_fin_ult1        False\n",
       "ind_cno_fin_ult1         False\n",
       "ind_ctju_fin_ult1        False\n",
       "ind_ctma_fin_ult1        False\n",
       "ind_ctop_fin_ult1        False\n",
       "ind_ctpp_fin_ult1        False\n",
       "ind_deco_fin_ult1        False\n",
       "ind_deme_fin_ult1        False\n",
       "ind_dela_fin_ult1        False\n",
       "ind_ecue_fin_ult1        False\n",
       "ind_fond_fin_ult1        False\n",
       "ind_hip_fin_ult1         False\n",
       "ind_plan_fin_ult1        False\n",
       "ind_pres_fin_ult1        False\n",
       "ind_reca_fin_ult1        False\n",
       "ind_tjcr_fin_ult1        False\n",
       "ind_valo_fin_ult1        False\n",
       "ind_viv_fin_ult1         False\n",
       "ind_nomina_ult1          False\n",
       "ind_nom_pens_ult1        False\n",
       "ind_recibo_ult1          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.isnull().any()\n",
    "# now we have cleaned all missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>sexo</th>\n",
       "      <th>age</th>\n",
       "      <th>fecha_alta</th>\n",
       "      <th>ind_nuevo</th>\n",
       "      <th>antiguedad</th>\n",
       "      <th>indrel</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1</th>\n",
       "      <th>ind_plan_fin_ult1</th>\n",
       "      <th>ind_pres_fin_ult1</th>\n",
       "      <th>ind_reca_fin_ult1</th>\n",
       "      <th>ind_tjcr_fin_ult1</th>\n",
       "      <th>ind_valo_fin_ult1</th>\n",
       "      <th>ind_viv_fin_ult1</th>\n",
       "      <th>ind_nomina_ult1</th>\n",
       "      <th>ind_nom_pens_ult1</th>\n",
       "      <th>ind_recibo_ult1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-28</td>\n",
       "      <td>1217174</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>2013-11-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-10-28</td>\n",
       "      <td>1217176</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>2013-11-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-10-28</td>\n",
       "      <td>1217173</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>2013-11-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-10-28</td>\n",
       "      <td>1217172</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>2013-11-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-10-28</td>\n",
       "      <td>1217171</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25</td>\n",
       "      <td>2013-11-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  fecha_dato  ncodpers ind_empleado pais_residencia  sexo  age fecha_alta  \\\n",
       "0 2015-10-28   1217174            N              ES   1.0   22 2013-11-08   \n",
       "1 2015-10-28   1217176            N              ES   1.0   32 2013-11-08   \n",
       "2 2015-10-28   1217173            N              ES   0.0   23 2013-11-08   \n",
       "3 2015-10-28   1217172            N              ES   0.0   32 2013-11-08   \n",
       "4 2015-10-28   1217171            N              ES   1.0   25 2013-11-08   \n",
       "\n",
       "   ind_nuevo antiguedad  indrel  ... ind_hip_fin_ult1 ind_plan_fin_ult1  \\\n",
       "0        0.0         23     1.0  ...                0                 0   \n",
       "1        0.0         23     1.0  ...                0                 0   \n",
       "2        0.0         23     1.0  ...                0                 0   \n",
       "3        0.0         23     1.0  ...                0                 0   \n",
       "4        0.0         23     1.0  ...                0                 0   \n",
       "\n",
       "  ind_pres_fin_ult1 ind_reca_fin_ult1 ind_tjcr_fin_ult1 ind_valo_fin_ult1  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 1                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "  ind_viv_fin_ult1 ind_nomina_ult1  ind_nom_pens_ult1 ind_recibo_ult1  \n",
       "0                0               0                  0               0  \n",
       "1                0               1                  1               0  \n",
       "2                0               0                  0               0  \n",
       "3                0               1                  1               1  \n",
       "4                0               0                  0               0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before optimization: 7.99 GB\n",
      "Number of columns before optimization: 45\n",
      "\n",
      "Optimizing 25 integer columns...\n",
      "Optimizing 4 float columns...\n",
      "Checking 14 object columns...\n",
      "  ‚Üí Converted 14 columns to category type\n",
      "\n",
      "Memory after optimization: 0.71 GB\n",
      "Number of columns after optimization: 45\n",
      "Memory saved: 91.1%\n"
     ]
    }
   ],
   "source": [
    "def optimize_dtypes(df):\n",
    "    \"\"\"\n",
    "    Optimize DataFrame dtypes to save memory.\n",
    "    Note: This function modifies df inplace, does not create new columns.\n",
    "    \"\"\"\n",
    "    print(f\"Memory before optimization: {df.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "    print(f\"Number of columns before optimization: {len(df.columns)}\")\n",
    "    initial_memory = df.memory_usage(deep=True).sum()\n",
    "\n",
    "    # 1. Optimize integer columns\n",
    "    int_cols = df.select_dtypes(include=['int64', 'int32']).columns\n",
    "    print(f\"\\nOptimizing {len(int_cols)} integer columns...\")\n",
    "    for col in int_cols:\n",
    "        col_min = df[col].min()\n",
    "        col_max = df[col].max()\n",
    "        if col_min >= 0:  # unsigned integer\n",
    "            if col_max < 255:\n",
    "                df[col] = df[col].astype('uint8')\n",
    "            elif col_max < 65535:\n",
    "                df[col] = df[col].astype('uint16')\n",
    "            else:\n",
    "                df[col] = df[col].astype('uint32')\n",
    "        else:  # signed integer\n",
    "            if col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype('int8')\n",
    "            elif col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype('int16')\n",
    "            else:\n",
    "                df[col] = df[col].astype('int32')\n",
    "\n",
    "    # 2. Optimize float columns\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    print(f\"Optimizing {len(float_cols)} float columns...\")\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].astype('float32')\n",
    "\n",
    "    # 3. Convert repeated string columns to category type\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns\n",
    "    print(f\"Checking {len(obj_cols)} object columns...\")\n",
    "    converted = 0\n",
    "    for col in obj_cols:\n",
    "        num_unique = df[col].nunique()\n",
    "        num_total = len(df[col])\n",
    "        if num_unique / num_total < 0.5:  # if unique values < 50%\n",
    "            df[col] = df[col].astype('category')\n",
    "            converted += 1\n",
    "    print(f\"  ‚Üí Converted {converted} columns to category type\")\n",
    "\n",
    "    # 4. Check that no columns were added\n",
    "    final_memory = df.memory_usage(deep=True).sum()\n",
    "    print(f\"\\nMemory after optimization: {final_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"Number of columns after optimization: {len(df.columns)}\")\n",
    "    print(f\"Memory saved: {(1 - final_memory / initial_memory) * 100:.1f}%\")\n",
    "    return df\n",
    "\n",
    "df_all = optimize_dtypes(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5OVdeHIg7uW",
    "outputId": "6eed9320-d7bb-4fa1-dc64-8a65e4d83f78",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total dataset: (8261972, 45)\n",
      "date: 2015-10-28 00:00:00 to 2016-06-28 00:00:00\n",
      "product_cols: 24\n"
     ]
    }
   ],
   "source": [
    "print(f\"total dataset: {df_all.shape}\")\n",
    "print(f\"date: {df_all['fecha_dato'].min()} to {df_all['fecha_dato'].max()}\")\n",
    "product_cols = [col for col in df_all.columns if col.startswith('ind_') and 'ult1' in col]\n",
    "print(f\"product_cols: {len(product_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:60: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  age_mean = df_all.groupby('age_bucket')['renta'].transform('mean')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:66: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  channel_mean = df_all.groupby('canal_entrada')['renta'].transform('mean')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_ones_5m'] = roll.reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_zeros_5m'] = (5 - roll).reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_delta'] = (series - prev).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_ones_5m'] = roll.reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_zeros_5m'] = (5 - roll).reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_delta'] = (series - prev).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_ones_5m'] = roll.reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_zeros_5m'] = (5 - roll).reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_delta'] = (series - prev).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_ones_5m'] = roll.reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_zeros_5m'] = (5 - roll).reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_delta'] = (series - prev).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_ones_5m'] = roll.reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_zeros_5m'] = (5 - roll).reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_delta'] = (series - prev).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_ones_5m'] = roll.reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_zeros_5m'] = (5 - roll).reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_delta'] = (series - prev).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_ones_5m'] = roll.reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_zeros_5m'] = (5 - roll).reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_delta'] = (series - prev).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_ones_5m'] = roll.reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_zeros_5m'] = (5 - roll).reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_delta'] = (series - prev).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_ones_5m'] = roll.reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_zeros_5m'] = (5 - roll).reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_delta'] = (series - prev).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_ones_5m'] = roll.reset_index(level=0, drop=True).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/374521033.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_zeros_5m'] = (5 - roll).reset_index(level=0, drop=True).astype('int8')\n"
     ]
    }
   ],
   "source": [
    "# 1. Âü∫Á°ÄÊéíÂ∫èÂíåÊó∂Èó¥ÁâπÂæÅÔºà‰øùÊåÅ‰∏çÂèòÔºåËøôÈÉ®ÂàÜÂ∑≤ÁªèÂæàÂø´Ôºâ\n",
    "df_all = df_all.sort_values(['ncodpers','fecha_dato']).reset_index(drop=True)\n",
    "df_all['month'] = df_all['fecha_dato'].dt.month.astype('int8')\n",
    "\n",
    "# 2. ‰∏ÄÊ¨°ÊÄßËé∑ÂèñÊâÄÊúâ‰∫ßÂìÅÁöÑÊï∞ÊçÆÁü©ÈòµÔºàÈÅøÂÖçÂæ™ÁéØÔºâ\n",
    "products_array = df_all[product_cols].values.astype('float32')\n",
    "customer_idx = df_all['ncodpers'].values\n",
    "unique_customers = np.unique(customer_idx)\n",
    "customer_map = {cid: i for i, cid in enumerate(unique_customers)}\n",
    "mapped_idx = np.array([customer_map[cid] for cid in customer_idx])\n",
    "\n",
    "# 3. ÊâπÈáèËÆ°ÁÆó EWM ÁâπÂæÅ\n",
    "def batch_ewm(array, alpha):\n",
    "    \"\"\"ÊâπÈáèËÆ°ÁÆóÂ§öÂàóÁöÑ EWM\"\"\"\n",
    "    w = 1\n",
    "    n = len(array)\n",
    "    weights = np.zeros(n)\n",
    "    weights[0] = w\n",
    "    for i in range(1, n):\n",
    "        w *= (1 - alpha)\n",
    "        weights[i] = w\n",
    "    weights = weights[::-1]  # ÂèçËΩ¨ÊùÉÈáç\n",
    "    weights /= weights.sum()\n",
    "    # ‰ΩøÁî®Âç∑ÁßØËÆ°ÁÆóÁßªÂä®Âä†ÊùÉÂπ≥Âùá\n",
    "    from numpy.lib.stride_tricks import as_strided\n",
    "    s = array.strides[0]\n",
    "    strided = as_strided(array, shape=(n-n+1, n), strides=(s, s))\n",
    "    return (strided * weights).sum(axis=1)\n",
    "\n",
    "# 4. Âπ¥ÈæÑÂíåÂÖ•Ë°åÊó∂Èó¥ÁâπÂæÅÔºà‰øùÊåÅ‰∏çÂèòÔºâ\n",
    "y = df_all['fecha_dato'].dt.year\n",
    "m = df_all['fecha_dato'].dt.month\n",
    "y0 = y.iloc[0]; m0 = m.iloc[0]\n",
    "df_all['rel_month'] = ((y - y0) * 12 + (m - m0)).astype('int16')\n",
    "\n",
    "df_all['fecha_alta'] = pd.to_datetime(df_all['fecha_alta'], errors='coerce')\n",
    "df_all['tenure_m'] = (\n",
    "    (df_all['fecha_dato'].dt.year - df_all['fecha_alta'].dt.year) * 12 +\n",
    "    (df_all['fecha_dato'].dt.month - df_all['fecha_alta'].dt.month)\n",
    ").clip(lower=0).fillna(0).astype('int16')\n",
    "\n",
    "# 5. Êî∂ÂÖ•ÂíåÂπ¥ÈæÑÁâπÂæÅÔºàÂêëÈáèÂåñÂ§ÑÁêÜÔºâ\n",
    "df_all['age'] = pd.to_numeric(df_all['age'], errors='coerce')\n",
    "df_all['renta'] = pd.to_numeric(df_all['renta'], errors='coerce')\n",
    "\n",
    "age_bins = [0,25,35,45,55,65,200]\n",
    "age_labels = ['<=25','26-35','36-45','46-55','56-65','66+']\n",
    "df_all['age_bucket'] = pd.cut(df_all['age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# 6. Êî∂ÂÖ•ÊØîÁéáÁâπÂæÅÔºà‰ΩøÁî® numpy ËøêÁÆóÂä†ÈÄüÔºâ\n",
    "renta_arr = df_all['renta'].values  # ÊèêÂâçËé∑ÂèñrentaÊï∞ÁªÑ\n",
    "\n",
    "if {'pais_residencia','provincia'}.issubset(df_all.columns):\n",
    "    loc_mean = df_all.groupby(['pais_residencia','provincia'])['renta'].transform('mean')\n",
    "    loc_mean_arr = loc_mean.values\n",
    "    ratio = np.divide(renta_arr, loc_mean_arr, out=np.ones_like(renta_arr), where=loc_mean_arr!=0)\n",
    "    df_all['renta_to_loc_mean'] = ratio.clip(0, 10).astype('float32')\n",
    "\n",
    "if 'age_bucket' in df_all.columns:\n",
    "    age_mean = df_all.groupby('age_bucket')['renta'].transform('mean')\n",
    "    age_mean_arr = age_mean.values\n",
    "    ratio = np.divide(renta_arr, age_mean_arr, out=np.ones_like(renta_arr)*-1, where=age_mean_arr!=0)\n",
    "    df_all['renta_to_age_mean'] = ratio.clip(-1, 10).astype('float32')\n",
    "\n",
    "if 'canal_entrada' in df_all.columns:\n",
    "    channel_mean = df_all.groupby('canal_entrada')['renta'].transform('mean')\n",
    "    channel_mean_arr = channel_mean.values\n",
    "    ratio = np.divide(renta_arr, channel_mean_arr, out=np.ones_like(renta_arr)*-1, where=channel_mean_arr!=0)\n",
    "    df_all['renta_to_channel_mean'] = ratio.clip(-1, 10).astype('float32')\n",
    "\n",
    "# 7. ÂéÜÂè≤Áä∂ÊÄÅÁâπÂæÅÔºàÊâπÈáèËÆ°ÁÆóÔºâ\n",
    "for p in product_cols:\n",
    "    # ËÆ°ÁÆóÂâç1/2/3ÊúàÁä∂ÊÄÅ\n",
    "    series = df_all[p]\n",
    "    for k in range(1, 4):\n",
    "        shifted = series.shift(k).fillna(0)\n",
    "        df_all[f'{p}_prev_{k}'] = shifted.astype('int8')\n",
    "    \n",
    "    # ËÆ°ÁÆóÂèòÂåñÂíåÊåÅÁª≠Êó∂Èïø\n",
    "    prev = df_all[f'{p}_prev_1']\n",
    "    df_all[f'{p}_delta'] = (series - prev).astype('int8')\n",
    "    \n",
    "    # ‰ΩøÁî® rolling ËÆ°ÁÆóËøë5ÊúàÁªüËÆ°\n",
    "    roll = prev.groupby(df_all['ncodpers']).rolling(5, min_periods=1).sum()\n",
    "    df_all[f'{p}_ones_5m'] = roll.reset_index(level=0, drop=True).astype('int8')\n",
    "    df_all[f'{p}_zeros_5m'] = (5 - roll).reset_index(level=0, drop=True).astype('int8')\n",
    "\n",
    "# 8. ËΩ¨Êç¢ÂàÜÁ±ªÂèòÈáèÔºà‰øùÊåÅ‰∏çÂèòÔºâ\n",
    "for col in ['segmento','canal_entrada','pais_residencia','provincia','age_bucket']:\n",
    "    if col in df_all.columns:\n",
    "        df_all[col] = df_all[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'{p}_prev'] = prev\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all['prev_products_code'] = (prev_matrix.values @ weights).astype('int64')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2663688485.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all['prev_products_code_bucket'] = (df_all['prev_products_code'] % 1000).astype('category')\n"
     ]
    }
   ],
   "source": [
    "# ËÆ°ÁÆóÊØè‰∏™ÊúàÊØè‰∏™‰∫ßÂìÅÁöÑÊÄª‰ΩìÂÖàÈ™åÊ¶ÇÁéá\n",
    "per_m = df_all['fecha_dato'].dt.to_period('M')\n",
    "monthly_pop = {}\n",
    "\n",
    "for p in product_cols:\n",
    "    # ËÆ°ÁÆó‰∏äÊúàÊòØÂê¶ÊåÅÊúâÔºàÂ¶ÇÊûúÊ≤°ÊúâÔºåÂ§çÁî®ÂâçÈù¢Â∑≤ÁªèÁÆóÂ•ΩÁöÑÔºâ\n",
    "    if f'{p}_prev' not in df_all.columns:\n",
    "        prev = df_all.groupby('ncodpers')[p].shift(1).fillna(0).astype('int8')\n",
    "        df_all[f'{p}_prev'] = prev\n",
    "    else:\n",
    "        prev = df_all[f'{p}_prev']\n",
    "\n",
    "    # ÂΩìÊúà\"Êñ∞Â¢û\"Ê†áËÆ∞ÔºàÁî®‰∫éËÆ°ÁÆóÊúàÂ∫¶ÂÖàÈ™åÔºâ\n",
    "    added = ((df_all[p] == 1) & (prev == 0)).astype('int8')\n",
    "\n",
    "    # ËÆ°ÁÆóÊúàÂ∫¶Êñ∞Â¢ûÁéáÔºåÂêëÂêéÁßª1Êúà‰Ωú‰∏∫ÂÖàÈ™å\n",
    "    rate_by_m = added.groupby(per_m).mean().astype('float32')\n",
    "    monthly_pop[p] = rate_by_m.shift(1).fillna(rate_by_m.mean())\n",
    "    df_all[f'pop_prior_{p}'] = per_m.map(monthly_pop[p]).astype('float32')\n",
    "\n",
    "# ‰∏äÊúà24‰∏™‰∫ßÂìÅÁöÑ0/1ÁªÑÂêàÂéãÁº©ÁºñÁ†Å\n",
    "prev_matrix = df_all[[f'{p}_prev' for p in product_cols]].astype('int8')\n",
    "weights = np.array([1 << i for i in range(len(product_cols))], dtype=np.int64)\n",
    "df_all['prev_products_code'] = (prev_matrix.values @ weights).astype('int64')\n",
    "df_all['prev_products_code_bucket'] = (df_all['prev_products_code'] % 1000).astype('category')\n",
    "\n",
    "# ÂèØÈÄâÔºöÊØè‰∏™‰∫ßÂìÅÁªÑÂêàÁöÑËΩ¨ÂåñÁéáÁâπÂæÅ\n",
    "# grouped = df_all.groupby('prev_products_code')\n",
    "# for p in product_cols:\n",
    "#     tcol = f'{p}_target'\n",
    "#     if tcol in df_all.columns:\n",
    "#         rate = grouped[tcol].transform('mean').fillna(0)\n",
    "#         df_all[f'{p}_combo_rate'] = rate.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>sexo</th>\n",
       "      <th>age</th>\n",
       "      <th>fecha_alta</th>\n",
       "      <th>ind_nuevo</th>\n",
       "      <th>antiguedad</th>\n",
       "      <th>indrel</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_viv_fin_ult1_prev</th>\n",
       "      <th>pop_prior_ind_viv_fin_ult1</th>\n",
       "      <th>ind_nomina_ult1_prev</th>\n",
       "      <th>pop_prior_ind_nomina_ult1</th>\n",
       "      <th>ind_nom_pens_ult1_prev</th>\n",
       "      <th>pop_prior_ind_nom_pens_ult1</th>\n",
       "      <th>ind_recibo_ult1_prev</th>\n",
       "      <th>pop_prior_ind_recibo_ult1</th>\n",
       "      <th>prev_products_code</th>\n",
       "      <th>prev_products_code_bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>F</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>248</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009905</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021723</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-11-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>F</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050050</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117038</td>\n",
       "      <td>524548</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>F</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004922</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011237</td>\n",
       "      <td>524548</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>F</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005628</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010923</td>\n",
       "      <td>786692</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-02-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>F</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003211</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011081</td>\n",
       "      <td>786692</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 245 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  fecha_dato  ncodpers ind_empleado pais_residencia  sexo  age fecha_alta  \\\n",
       "0 2015-10-28     15889            F              ES   1.0   56 1995-01-16   \n",
       "1 2015-11-28     15889            F              ES   1.0   56 1995-01-16   \n",
       "2 2015-12-28     15889            F              ES   1.0   56 1995-01-16   \n",
       "3 2016-01-28     15889            F              ES   1.0   56 1995-01-16   \n",
       "4 2016-02-28     15889            F              ES   1.0   56 1995-01-16   \n",
       "\n",
       "   ind_nuevo antiguedad  indrel  ... ind_viv_fin_ult1_prev  \\\n",
       "0        0.0        248     1.0  ...                     0   \n",
       "1        0.0        249     1.0  ...                     0   \n",
       "2        0.0        250     1.0  ...                     0   \n",
       "3        0.0        251     1.0  ...                     0   \n",
       "4        0.0        252     1.0  ...                     0   \n",
       "\n",
       "  pop_prior_ind_viv_fin_ult1 ind_nomina_ult1_prev pop_prior_ind_nomina_ult1  \\\n",
       "0                   0.000386                    0                  0.009905   \n",
       "1                   0.003437                    0                  0.050050   \n",
       "2                   0.000006                    0                  0.004922   \n",
       "3                   0.000009                    0                  0.005628   \n",
       "4                   0.000002                    0                  0.002714   \n",
       "\n",
       "  ind_nom_pens_ult1_prev pop_prior_ind_nom_pens_ult1 ind_recibo_ult1_prev  \\\n",
       "0                      0                    0.010971                    0   \n",
       "1                      0                    0.054801                    0   \n",
       "2                      0                    0.004939                    0   \n",
       "3                      0                    0.005714                    0   \n",
       "4                      0                    0.003211                    0   \n",
       "\n",
       "  pop_prior_ind_recibo_ult1  prev_products_code  prev_products_code_bucket  \n",
       "0                  0.021723                   0                          0  \n",
       "1                  0.117038              524548                        548  \n",
       "2                  0.011237              524548                        548  \n",
       "3                  0.010923              786692                        692  \n",
       "4                  0.011081              786692                        692  \n",
       "\n",
       "[5 rows x 245 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets built: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
      "/var/folders/q5/np1hsyq16cdfp8yvvx5qbhjm0000gn/T/ipykernel_1109/2999452671.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n"
     ]
    }
   ],
   "source": [
    "# product columns\n",
    "if 'product_cols' not in globals():\n",
    "    product_cols = [c for c in df_all.columns if c.startswith('ind_') and c.endswith('_ult1')]\n",
    "\n",
    "# previous month product state\n",
    "for p in product_cols:\n",
    "    prev_col = f'{p}_prev'\n",
    "    if prev_col not in df_all.columns:\n",
    "        df_all[prev_col] = df_all.groupby('ncodpers')[p].shift(1).fillna(0).astype('int8')\n",
    "\n",
    "# target column is whether the product is newly added this month\n",
    "target_cols = []\n",
    "for p in product_cols:\n",
    "    tcol = f'{p}_target'\n",
    "    df_all[tcol] = ((df_all[p] == 1) & (df_all[f'{p}_prev'] == 0)).astype('int8')\n",
    "    target_cols.append(tcol)\n",
    "\n",
    "print(f\"targets built: {len(target_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: (6400904, 269) (date: 2015-10-28 00:00:00 to 2016-04-28 00:00:00)\n",
      "validation set: (931453, 269) (date: 2016-05-28)\n",
      "test set: (929615, 269) (date: 2016-06-28)\n"
     ]
    }
   ],
   "source": [
    "test_date = '2016-06-28'\n",
    "val_date = '2016-05-28'\n",
    "\n",
    "train_df = df_all[df_all['fecha_dato'] < val_date].copy()\n",
    "val_df = df_all[df_all['fecha_dato'] == val_date].copy()\n",
    "test_df = df_all[df_all['fecha_dato'] == test_date].copy()\n",
    "\n",
    "print(f\"training set: {train_df.shape} (date: {train_df['fecha_dato'].min()} to {train_df['fecha_dato'].max()})\")\n",
    "print(f\"validation set: {val_df.shape} (date: {val_date})\")\n",
    "print(f\"test set: {test_df.shape} (date: {test_date})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>sexo</th>\n",
       "      <th>age</th>\n",
       "      <th>fecha_alta</th>\n",
       "      <th>ind_nuevo</th>\n",
       "      <th>antiguedad</th>\n",
       "      <th>indrel</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_target</th>\n",
       "      <th>ind_plan_fin_ult1_target</th>\n",
       "      <th>ind_pres_fin_ult1_target</th>\n",
       "      <th>ind_reca_fin_ult1_target</th>\n",
       "      <th>ind_tjcr_fin_ult1_target</th>\n",
       "      <th>ind_valo_fin_ult1_target</th>\n",
       "      <th>ind_viv_fin_ult1_target</th>\n",
       "      <th>ind_nomina_ult1_target</th>\n",
       "      <th>ind_nom_pens_ult1_target</th>\n",
       "      <th>ind_recibo_ult1_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>F</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>248</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-11-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>F</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>F</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>F</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-02-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>F</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 269 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  fecha_dato  ncodpers ind_empleado pais_residencia  sexo  age fecha_alta  \\\n",
       "0 2015-10-28     15889            F              ES   1.0   56 1995-01-16   \n",
       "1 2015-11-28     15889            F              ES   1.0   56 1995-01-16   \n",
       "2 2015-12-28     15889            F              ES   1.0   56 1995-01-16   \n",
       "3 2016-01-28     15889            F              ES   1.0   56 1995-01-16   \n",
       "4 2016-02-28     15889            F              ES   1.0   56 1995-01-16   \n",
       "\n",
       "   ind_nuevo antiguedad  indrel  ... ind_hip_fin_ult1_target  \\\n",
       "0        0.0        248     1.0  ...                       0   \n",
       "1        0.0        249     1.0  ...                       0   \n",
       "2        0.0        250     1.0  ...                       0   \n",
       "3        0.0        251     1.0  ...                       0   \n",
       "4        0.0        252     1.0  ...                       0   \n",
       "\n",
       "  ind_plan_fin_ult1_target ind_pres_fin_ult1_target ind_reca_fin_ult1_target  \\\n",
       "0                        0                        0                        0   \n",
       "1                        0                        0                        0   \n",
       "2                        0                        0                        0   \n",
       "3                        0                        0                        0   \n",
       "4                        0                        0                        0   \n",
       "\n",
       "  ind_tjcr_fin_ult1_target ind_valo_fin_ult1_target ind_viv_fin_ult1_target  \\\n",
       "0                        0                        1                       0   \n",
       "1                        0                        0                       0   \n",
       "2                        1                        0                       0   \n",
       "3                        0                        0                       0   \n",
       "4                        0                        0                       0   \n",
       "\n",
       "  ind_nomina_ult1_target  ind_nom_pens_ult1_target  ind_recibo_ult1_target  \n",
       "0                      0                         0                       0  \n",
       "1                      0                         0                       0  \n",
       "2                      0                         0                       0  \n",
       "3                      0                         0                       0  \n",
       "4                      0                         0                       0  \n",
       "\n",
       "[5 rows x 269 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count: 193\n"
     ]
    }
   ],
   "source": [
    "# select feature columns (exclude target and product cols)\n",
    "# ncodpers is user id, fecha_dato is date, fecha_alta is join date, ult_fec_cli_1t is last date of first product (dropped)\n",
    "leak_cols = product_cols + target_cols + ['ncodpers','fecha_dato','fecha_alta','ult_fec_cli_1t']\n",
    "feature_cols = [c for c in df_all.columns if c not in leak_cols]\n",
    "feature_cols = [c for c in feature_cols if not c.endswith('_delta')]\n",
    "\n",
    "print('Feature count:', len(feature_cols))\n",
    "\n",
    "X_train = train_df[feature_cols].copy()\n",
    "X_val   = val_df[feature_cols].copy()\n",
    "X_test  = test_df[feature_cols].copy()\n",
    "\n",
    "y_train = train_df[target_cols].copy()\n",
    "y_val   = val_df[target_cols].copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.253626\n",
      "[100]\tvalid's auc: 0.511127\n",
      "[150]\tvalid's auc: 0.274814\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid's auc: 0.599855\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.991399\n",
      "[100]\tvalid's auc: 0.991696\n",
      "[150]\tvalid's auc: 0.991843\n",
      "[200]\tvalid's auc: 0.991893\n",
      "[250]\tvalid's auc: 0.991963\n",
      "[300]\tvalid's auc: 0.992022\n",
      "[350]\tvalid's auc: 0.992064\n",
      "[400]\tvalid's auc: 0.992076\n",
      "[450]\tvalid's auc: 0.992072\n",
      "Early stopping, best iteration is:\n",
      "[385]\tvalid's auc: 0.992084\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.651063\n",
      "[100]\tvalid's auc: 0.651352\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid's auc: 0.70499\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.961882\n",
      "[100]\tvalid's auc: 0.962888\n",
      "[150]\tvalid's auc: 0.963159\n",
      "[200]\tvalid's auc: 0.963274\n",
      "[250]\tvalid's auc: 0.963374\n",
      "[300]\tvalid's auc: 0.963509\n",
      "[350]\tvalid's auc: 0.963608\n",
      "[400]\tvalid's auc: 0.963681\n",
      "[450]\tvalid's auc: 0.963701\n",
      "[500]\tvalid's auc: 0.963635\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[434]\tvalid's auc: 0.963722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.999954\n",
      "[100]\tvalid's auc: 0.999972\n",
      "[150]\tvalid's auc: 0.999983\n",
      "[200]\tvalid's auc: 0.99998\n",
      "Early stopping, best iteration is:\n",
      "[146]\tvalid's auc: 0.999984\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.991064\n",
      "[100]\tvalid's auc: 0.991542\n",
      "[150]\tvalid's auc: 0.992833\n",
      "[200]\tvalid's auc: 0.993112\n",
      "[250]\tvalid's auc: 0.993423\n",
      "[300]\tvalid's auc: 0.993549\n",
      "[350]\tvalid's auc: 0.993818\n",
      "[400]\tvalid's auc: 0.993787\n",
      "[450]\tvalid's auc: 0.99368\n",
      "Early stopping, best iteration is:\n",
      "[380]\tvalid's auc: 0.993849\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.9753\n",
      "[100]\tvalid's auc: 0.97573\n",
      "[150]\tvalid's auc: 0.976696\n",
      "[200]\tvalid's auc: 0.976927\n",
      "[250]\tvalid's auc: 0.976822\n",
      "[300]\tvalid's auc: 0.976959\n",
      "[350]\tvalid's auc: 0.977093\n",
      "[400]\tvalid's auc: 0.977372\n",
      "[450]\tvalid's auc: 0.977594\n",
      "[500]\tvalid's auc: 0.977639\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[470]\tvalid's auc: 0.977676\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.970247\n",
      "[100]\tvalid's auc: 0.971883\n",
      "[150]\tvalid's auc: 0.972194\n",
      "[200]\tvalid's auc: 0.971882\n",
      "Early stopping, best iteration is:\n",
      "[128]\tvalid's auc: 0.972318\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.94184\n",
      "[100]\tvalid's auc: 0.944406\n",
      "[150]\tvalid's auc: 0.944195\n",
      "[200]\tvalid's auc: 0.943725\n",
      "Early stopping, best iteration is:\n",
      "[104]\tvalid's auc: 0.944681\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.961419\n",
      "[100]\tvalid's auc: 0.961777\n",
      "[150]\tvalid's auc: 0.96177\n",
      "[200]\tvalid's auc: 0.961589\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid's auc: 0.961828\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.876924\n",
      "[100]\tvalid's auc: 0.879862\n",
      "[150]\tvalid's auc: 0.880962\n",
      "[200]\tvalid's auc: 0.880772\n",
      "[250]\tvalid's auc: 0.880036\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid's auc: 0.881816\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.204113\n",
      "[100]\tvalid's auc: 0.295319\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.494058\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.708558\n",
      "[100]\tvalid's auc: 0.692602\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.796522\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.273462\n",
      "[100]\tvalid's auc: 0.270237\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.693575\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.860254\n",
      "[100]\tvalid's auc: 0.859591\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.860924\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.969865\n",
      "[100]\tvalid's auc: 0.970467\n",
      "[150]\tvalid's auc: 0.970749\n",
      "[200]\tvalid's auc: 0.970892\n",
      "[250]\tvalid's auc: 0.970931\n",
      "[300]\tvalid's auc: 0.970958\n",
      "[350]\tvalid's auc: 0.970953\n",
      "[400]\tvalid's auc: 0.970875\n",
      "Early stopping, best iteration is:\n",
      "[310]\tvalid's auc: 0.970967\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.886319\n",
      "[100]\tvalid's auc: 0.889564\n",
      "[150]\tvalid's auc: 0.890754\n",
      "[200]\tvalid's auc: 0.890673\n",
      "Early stopping, best iteration is:\n",
      "[138]\tvalid's auc: 0.891066\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.40404\n",
      "[100]\tvalid's auc: 0.494888\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.630741\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.96865\n",
      "[100]\tvalid's auc: 0.96896\n",
      "[150]\tvalid's auc: 0.969102\n",
      "[200]\tvalid's auc: 0.969061\n",
      "[250]\tvalid's auc: 0.968998\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid's auc: 0.969118\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.965806\n",
      "[100]\tvalid's auc: 0.966173\n",
      "[150]\tvalid's auc: 0.966388\n",
      "[200]\tvalid's auc: 0.966408\n",
      "[250]\tvalid's auc: 0.966393\n",
      "[300]\tvalid's auc: 0.966389\n",
      "Early stopping, best iteration is:\n",
      "[219]\tvalid's auc: 0.966442\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's auc: 0.963796\n",
      "[100]\tvalid's auc: 0.96417\n",
      "[150]\tvalid's auc: 0.96436\n",
      "[200]\tvalid's auc: 0.964513\n",
      "[250]\tvalid's auc: 0.964625\n",
      "[300]\tvalid's auc: 0.964717\n",
      "[350]\tvalid's auc: 0.964747\n",
      "[400]\tvalid's auc: 0.964739\n",
      "[450]\tvalid's auc: 0.964732\n",
      "Early stopping, best iteration is:\n",
      "[368]\tvalid's auc: 0.964761\n",
      "Avg valid AUC across trained products: 0.8624 (trained 21/24)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# convert feature columns to 'category' dtype for LightGBM\n",
    "categorical_cols = [c for c in feature_cols\n",
    "                    if (X_train[c].dtype == 'object') or (str(X_train[c].dtype) == 'category')]\n",
    "\n",
    "for c in categorical_cols:\n",
    "    X_train[c] = X_train[c].astype('category')\n",
    "    X_val[c]   = X_val[c].astype('category')\n",
    "    X_test[c]  = X_test[c].astype('category')\n",
    "\n",
    "# 3) get all product names (without _target suffix)\n",
    "all_products = [c.replace('_target','') for c in target_cols]\n",
    "\n",
    "# 4) store model and validation AUC for each product\n",
    "models  = {}  \n",
    "metrics = {}  \n",
    "\n",
    "# 5) loop over each product to train a separate model for 24 products\n",
    "for product in all_products:\n",
    "    target_col = f'{product}_target'         # e.g. 'ind_ahor_fin_ult1_target'\n",
    "    y_tr = y_train[target_col]              \n",
    "    y_va = y_val[target_col]                 \n",
    "\n",
    "    # skip products that are too rare in training or validation\n",
    "    pos = int(y_tr.sum())                    # positive samples in training\n",
    "    if pos < 10 or y_va.sum() == 0:           # threshold can be adjusted\n",
    "        # record as skipped for later inspection (optional)\n",
    "        metrics[product] = {'val_auc': None, 'skipped': True}\n",
    "        continue\n",
    "\n",
    "    # 6) construct LightGBM dataset objects and declare categorical_feature\n",
    "    dtrain = lgb.Dataset(\n",
    "        X_train,                              # training features\n",
    "        label=y_tr,                           # training labels (0/1)\n",
    "        categorical_feature=categorical_cols, # tell LGB which columns are categorical\n",
    "        free_raw_data=False                   # keep raw data for later use\n",
    "    )\n",
    "    dvalid = lgb.Dataset(\n",
    "        X_val,                                # validation features\n",
    "        label=y_va,                           # validation labels\n",
    "        categorical_feature=categorical_cols, # same as above\n",
    "        reference=dtrain,                     # share dictionary info to save memory\n",
    "        free_raw_data=False\n",
    "    )\n",
    "\n",
    "    # 7) LightGBM parameters with typical values for binary classification\n",
    "    params = dict(\n",
    "        objective='binary',\n",
    "        metric='auc',\n",
    "        learning_rate=0.01,      # ‰ªé 0.03 Êîπ‰∏∫ 0.01\n",
    "        num_leaves=127,          # ‰ªé 31 Êîπ‰∏∫ 127\n",
    "        max_depth=8,             # ‰ªé 6 Êîπ‰∏∫ 8\n",
    "        feature_fraction=0.7,    # ‰ªé 0.8 Êîπ‰∏∫ 0.7\n",
    "        bagging_fraction=0.7,    # ‰ªé 0.8 Êîπ‰∏∫ 0.7\n",
    "        bagging_freq=1,          # ‰ªé 5 Êîπ‰∏∫ 1\n",
    "        min_child_samples=50,    # ‰ªé 20 Êîπ‰∏∫ 50\n",
    "        reg_alpha=0.1,           # Êñ∞Â¢û\n",
    "        reg_lambda=0.1,          # Êñ∞Â¢û\n",
    "        is_unbalance=True,\n",
    "        num_threads=-1,\n",
    "        seed=2027,\n",
    "        verbose=-1\n",
    ")\n",
    "    # 8) train the model with early stopping (stop if no improvement on validation set for 50 rounds)\n",
    "    model = lgb.train(\n",
    "        params=params,                    # parameters\n",
    "        train_set=dtrain,                 # training data\n",
    "        num_boost_round=500,              # maximum rounds (can be truncated with early stopping)\n",
    "        valid_sets=[dvalid],              # validation set\n",
    "        valid_names=['valid'],            # validation set name\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(100),      # stop if no improvement on validation set for 50 rounds\n",
    "            lgb.log_evaluation(50)       # print AUC every 50 rounds\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 9) save model\n",
    "    models[product] = model\n",
    "\n",
    "    # 10) make a prediction on the validation set and calculate AUC (for monitoring)\n",
    "    y_pred_val = model.predict(\n",
    "        X_val,                             # validation features\n",
    "        num_iteration=model.best_iteration # use best iteration\n",
    "    )\n",
    "    val_auc = roc_auc_score(y_va, y_pred_val)  # calculate AUC\n",
    "    metrics[product] = {'val_auc': float(val_auc)}  # record validation AUC for this product\n",
    "\n",
    "# 11) (optional) view overall AUC summary\n",
    "#     filter out skipped products and calculate average AUC as reference\n",
    "valid_aucs = [m['val_auc'] for m in metrics.values() if m.get('val_auc') is not None]\n",
    "if valid_aucs:\n",
    "    print(f\"Avg valid AUC across trained products: {np.mean(valid_aucs):.4f} \"\n",
    "          f\"(trained {len(valid_aucs)}/{len(all_products)})\")\n",
    "else:\n",
    "    print(\"No products were trained (likely all too rare in training/validation).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation metrics: map@7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-input": true,
    "id": "2iVZ6oSCg7ue",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# referring to https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "\n",
    "#average precision at k\n",
    "def apk(actual, predicted, k=7):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=7):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted\n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_dato</th>\n",
       "      <th>ncodpers</th>\n",
       "      <th>ind_empleado</th>\n",
       "      <th>pais_residencia</th>\n",
       "      <th>sexo</th>\n",
       "      <th>age</th>\n",
       "      <th>fecha_alta</th>\n",
       "      <th>ind_nuevo</th>\n",
       "      <th>antiguedad</th>\n",
       "      <th>indrel</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_hip_fin_ult1_target</th>\n",
       "      <th>ind_plan_fin_ult1_target</th>\n",
       "      <th>ind_pres_fin_ult1_target</th>\n",
       "      <th>ind_reca_fin_ult1_target</th>\n",
       "      <th>ind_tjcr_fin_ult1_target</th>\n",
       "      <th>ind_valo_fin_ult1_target</th>\n",
       "      <th>ind_viv_fin_ult1_target</th>\n",
       "      <th>ind_nomina_ult1_target</th>\n",
       "      <th>ind_nom_pens_ult1_target</th>\n",
       "      <th>ind_recibo_ult1_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>15889</td>\n",
       "      <td>F</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>15890</td>\n",
       "      <td>A</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>15892</td>\n",
       "      <td>F</td>\n",
       "      <td>ES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>15893</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63</td>\n",
       "      <td>1997-10-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>15894</td>\n",
       "      <td>A</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "      <td>1995-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926658</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>1548202</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926659</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>1548203</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926660</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>1548204</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926661</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>1548206</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926662</th>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>1548207</td>\n",
       "      <td>N</td>\n",
       "      <td>ES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>926663 rows √ó 269 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fecha_dato  ncodpers ind_empleado pais_residencia  sexo  age  \\\n",
       "0      2016-05-28     15889            F              ES   1.0   56   \n",
       "1      2016-05-28     15890            A              ES   1.0   63   \n",
       "2      2016-05-28     15892            F              ES   0.0   62   \n",
       "3      2016-05-28     15893            N              ES   1.0   63   \n",
       "4      2016-05-28     15894            A              ES   1.0   60   \n",
       "...           ...       ...          ...             ...   ...  ...   \n",
       "926658 2016-05-28   1548202            N              ES   1.0   22   \n",
       "926659 2016-05-28   1548203            N              ES   1.0   51   \n",
       "926660 2016-05-28   1548204            N              ES   1.0   54   \n",
       "926661 2016-05-28   1548206            N              ES   0.0   40   \n",
       "926662 2016-05-28   1548207            N              ES   1.0   32   \n",
       "\n",
       "       fecha_alta  ind_nuevo antiguedad  indrel  ... ind_hip_fin_ult1_target  \\\n",
       "0      1995-01-16        0.0        255     1.0  ...                       0   \n",
       "1      1995-01-16        0.0        256     1.0  ...                       0   \n",
       "2      1995-01-16        0.0        256     1.0  ...                       0   \n",
       "3      1997-10-03        0.0        256     1.0  ...                       0   \n",
       "4      1995-01-16        0.0        256     1.0  ...                       0   \n",
       "...           ...        ...        ...     ...  ...                     ...   \n",
       "926658 2016-04-29        1.0          1     1.0  ...                       0   \n",
       "926659 2016-04-29        1.0          1     1.0  ...                       0   \n",
       "926660 2016-04-29        1.0          1     1.0  ...                       0   \n",
       "926661 2016-04-29        1.0          1     1.0  ...                       0   \n",
       "926662 2016-04-29        1.0          1     1.0  ...                       0   \n",
       "\n",
       "       ind_plan_fin_ult1_target ind_pres_fin_ult1_target  \\\n",
       "0                             0                        0   \n",
       "1                             0                        0   \n",
       "2                             0                        0   \n",
       "3                             0                        0   \n",
       "4                             0                        0   \n",
       "...                         ...                      ...   \n",
       "926658                        0                        0   \n",
       "926659                        0                        0   \n",
       "926660                        0                        0   \n",
       "926661                        0                        0   \n",
       "926662                        0                        0   \n",
       "\n",
       "       ind_reca_fin_ult1_target ind_tjcr_fin_ult1_target  \\\n",
       "0                             0                        1   \n",
       "1                             0                        0   \n",
       "2                             0                        0   \n",
       "3                             0                        0   \n",
       "4                             0                        0   \n",
       "...                         ...                      ...   \n",
       "926658                        0                        0   \n",
       "926659                        0                        0   \n",
       "926660                        0                        0   \n",
       "926661                        0                        0   \n",
       "926662                        0                        0   \n",
       "\n",
       "       ind_valo_fin_ult1_target ind_viv_fin_ult1_target  \\\n",
       "0                             0                       0   \n",
       "1                             0                       0   \n",
       "2                             0                       0   \n",
       "3                             0                       0   \n",
       "4                             0                       0   \n",
       "...                         ...                     ...   \n",
       "926658                        0                       0   \n",
       "926659                        0                       0   \n",
       "926660                        0                       0   \n",
       "926661                        0                       0   \n",
       "926662                        0                       0   \n",
       "\n",
       "       ind_nomina_ult1_target  ind_nom_pens_ult1_target  \\\n",
       "0                           0                         0   \n",
       "1                           0                         0   \n",
       "2                           0                         0   \n",
       "3                           0                         0   \n",
       "4                           0                         0   \n",
       "...                       ...                       ...   \n",
       "926658                      0                         0   \n",
       "926659                      0                         0   \n",
       "926660                      0                         0   \n",
       "926661                      0                         0   \n",
       "926662                      0                         0   \n",
       "\n",
       "        ind_recibo_ult1_target  \n",
       "0                            0  \n",
       "1                            0  \n",
       "2                            0  \n",
       "3                            0  \n",
       "4                            0  \n",
       "...                        ...  \n",
       "926658                       0  \n",
       "926659                       0  \n",
       "926660                       0  \n",
       "926661                       0  \n",
       "926662                       0  \n",
       "\n",
       "[926663 rows x 269 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# choose validation and test date\n",
    "val_date   = '2016-04-28'\n",
    "eval_date  = '2016-05-28'   \n",
    "\n",
    "val_df_m   = df_all[df_all['fecha_dato'] == val_date].copy()\n",
    "eval_df_m  = df_all[df_all['fecha_dato'] == eval_date].copy()\n",
    "\n",
    "# only keep users that appear in both months and sort by ncodpers\n",
    "common_ids = np.intersect1d(val_df_m['ncodpers'].values, eval_df_m['ncodpers'].values)\n",
    "val_df_m   = val_df_m.loc[val_df_m['ncodpers'].isin(common_ids)].sort_values('ncodpers').reset_index(drop=True)\n",
    "eval_df_m  = eval_df_m.loc[eval_df_m['ncodpers'].isin(common_ids)].sort_values('ncodpers').reset_index(drop=True)\n",
    "\n",
    "eval_df_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feature matrix: in april\n",
    "X_val_m = val_df_m[feature_cols].copy()\n",
    "\n",
    "# prediction using april features\n",
    "all_products = [c.replace('_target','') for c in target_cols]\n",
    "P = len(all_products)\n",
    "N = len(X_val_m)\n",
    "\n",
    "val_scores = np.zeros((N, P), dtype=np.float32)\n",
    "for j, p in enumerate(all_products):\n",
    "    if p in models:\n",
    "        m = models[p]\n",
    "        val_scores[:, j] = m.predict(X_val_m, num_iteration=getattr(m, \"best_iteration\", None)).astype(np.float32)\n",
    "    else:\n",
    "        prior_col = f'pop_prior_{p}'\n",
    "        if prior_col in val_df_m.columns:\n",
    "            val_scores[:, j] = val_df_m[prior_col].to_numpy(dtype=np.float32)\n",
    "        else:\n",
    "            val_scores[:, j] = float(y_train[f'{p}_target'].mean()) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAP@7 (model only): 0.0149\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# moddel-based recommendation evaluation (MAP@7) ---\n",
    "# 1. Build a mask for products already owned in April (to avoid recommending them again)\n",
    "owned_mask = val_df_m[all_products].to_numpy(dtype=np.int8)  # 1 if owned in April, else 0\n",
    "\n",
    "prod_arr = np.array(all_products)\n",
    "y_eval_mat = eval_df_m[[f\"{p}_target\" for p in all_products]].to_numpy(dtype=np.int8)  #(926663, 24)\n",
    "actual_products = [prod_arr[np.where(r == 1)[0]].tolist() for r in y_eval_mat] #list of 926663\n",
    "\n",
    "# 4. Mask out products already owned in April by setting their score to a very low value\n",
    "val_scores_masked = val_scores.copy()\n",
    "val_scores_masked[owned_mask == 1] = -1e9  # ensure these will not be recommended\n",
    "\n",
    "# 5. For each user, select the top 7 products with the highest predicted scores\n",
    "topk_idx = np.argpartition(val_scores_masked, -7, axis=1)[:, -7:] \n",
    "row = np.arange(N)[:, None]\n",
    "order = np.argsort(val_scores_masked[row, topk_idx], axis=1)[:, ::-1]  # sort top 7 by score descending\n",
    "predicted_products = prod_arr[topk_idx[row, order]].tolist()  \n",
    "# 6. Compute MAP@7: mean average precision at 7 over all users\n",
    "val_map7 = mapk(actual_products, predicted_products, k=7)\n",
    "print(f\"Validation MAP@7 (model only): {val_map7:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.50, MAP@7=0.0153\n",
      "alpha=0.60, MAP@7=0.0152\n",
      "alpha=0.70, MAP@7=0.0151\n",
      "alpha=0.80, MAP@7=0.0150\n",
      "alpha=0.90, MAP@7=0.0149\n",
      "Best alpha: 0.5 MAP@7: 0.0153\n",
      "Validation MAP@7 (best alpha): 0.0153\n"
     ]
    }
   ],
   "source": [
    "# --- ÂÖàÂáÜÂ§áÂÖàÈ™åÁü©Èòµ„ÄÅÂ±èËîΩÁü©Èòµ„ÄÅÁúüÂÆûÊñ∞Â¢ûÔºàÈÉΩÂü∫‰∫é 4‚Üí5 ÁöÑËØÑ‰º∞Ôºâ ---\n",
    "prior_cols = [f'pop_prior_{p}' for p in all_products]\n",
    "assert set(prior_cols).issubset(val_df_m.columns), \"val_df_mÁº∫Â∞ëpop_prior_*Âàó\"\n",
    "prior_mat = val_df_m[prior_cols].to_numpy(dtype=np.float32)      # 4ÊúàÁöÑÂÖàÈ™å\n",
    "\n",
    "owned_mask = val_df_m[all_products].to_numpy(dtype=np.int8)      # 4ÊúàÂ∑≤ÊåÅÊúâÔºàÈ¢ÑÊµãËµ∑ÁÇπÔºâ\n",
    "\n",
    "prod_arr = np.array(all_products)\n",
    "y_eval_mat = eval_df_m[[f\"{p}_target\" for p in all_products]].to_numpy(dtype=np.int8)  # 5ÊúàÁöÑÊñ∞Â¢û\n",
    "actual_products = [prod_arr[np.where(r == 1)[0]].tolist() for r in y_eval_mat]\n",
    "\n",
    "def topk_from_scores(scores, k=7, product_names=None):\n",
    "    N, P = scores.shape\n",
    "    topk_idx = np.argpartition(scores, -k, axis=1)[:, -k:]\n",
    "    row = np.arange(N)[:, None]\n",
    "    order = np.argsort(scores[row, topk_idx], axis=1)[:, ::-1]\n",
    "    idx_sorted = topk_idx[row, order]\n",
    "    return np.array(product_names)[idx_sorted].tolist() if product_names is not None else idx_sorted\n",
    "\n",
    "# --- Œ± ÁΩëÊ†ºÊêúÁ¥¢ ---\n",
    "alphas = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "best_map, best_alpha = -1.0, None\n",
    "for a in alphas:\n",
    "    s = a * val_scores + (1.0 - a) * prior_mat      # ËûçÂêà\n",
    "    s[owned_mask == 1] = -1e9                       # Â±èËîΩ4ÊúàÂ∑≤ÊåÅÊúâ\n",
    "    pred = topk_from_scores(s, 7, product_names=all_products)\n",
    "    map7 = mapk(actual_products, pred, 7)\n",
    "    print(f'alpha={a:.2f}, MAP@7={map7:.4f}')\n",
    "    if map7 > best_map:\n",
    "        best_map, best_alpha = map7, a\n",
    "\n",
    "print('Best alpha:', best_alpha, 'MAP@7:', f'{best_map:.4f}')\n",
    "\n",
    "# --- Áî®ÊúÄ‰ºò Œ± ÁîüÊàêÊúÄÁªàÁöÑÈ™åËØÅÈ¢ÑÊµãÔºà‰æø‰∫éÂêéÁª≠ÂØπÊØî/Â≠òÊ°£Ôºâ ---\n",
    "alpha = best_alpha\n",
    "val_scores_blend = alpha * val_scores + (1.0 - alpha) * prior_mat\n",
    "val_scores_blend[owned_mask == 1] = -1e9\n",
    "topk_idx = np.argpartition(val_scores_blend, -7, axis=1)[:, -7:]\n",
    "row = np.arange(N)[:, None]\n",
    "order = np.argsort(val_scores_blend[row, topk_idx], axis=1)[:, ::-1]\n",
    "predicted_products = prod_arr[topk_idx[row, order]].tolist()\n",
    "\n",
    "val_map7 = mapk(actual_products, predicted_products, k=7)\n",
    "print(f\"Validation MAP@7 (best alpha): {val_map7:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szMMmPo3utJO"
   },
   "source": [
    "# Fairness Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYpEooD7qOih"
   },
   "source": [
    "## Fairness by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "60Yhi_F3qNmc",
    "outputId": "026eea61-e3dd-4c80-b6cc-9e8bad1a87e8"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Drop rows with missing or invalid age\n",
    "# merged_df['age'] = pd.to_numeric(merged_df['age'], errors='coerce')\n",
    "# age_df = merged_df.dropna(subset=['age'])\n",
    "# age_df = age_df[(age_df['age'] >= 18) & (age_df['age'] <= 100)]  # optional: realistic range\n",
    "\n",
    "# # Create age groups\n",
    "# age_df['age_group'] = pd.cut(age_df['age'], bins=[18, 30, 45, 60, 75, 100],\n",
    "#                              labels=['18-30', '31-45', '46-60', '61-75', '76+'])\n",
    "\n",
    "# # Group by age group and calculate mean AP@7 and number of users\n",
    "# age_summary = age_df.groupby('age_group')['apk'].agg(['mean', 'count']).reset_index()\n",
    "# age_summary.columns = ['age_group', 'mean_apk', 'n_users']\n",
    "\n",
    "# # Print summary table\n",
    "# print(age_summary)\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.bar(age_summary['age_group'].astype(str), age_summary['mean_apk'])\n",
    "# plt.xlabel('Age Group')\n",
    "# plt.ylabel('Mean AP@7')\n",
    "# plt.title('Recommendation Fairness by Age Group')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9Xan_vwcw4N"
   },
   "source": [
    "## Fairness by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "lt_TcCx7PgIY",
    "outputId": "4bea3ce0-99f3-4a26-f65c-d7ace7cf6b96"
   },
   "outputs": [],
   "source": [
    "# # delete missing values\n",
    "# gender_df = merged_df.dropna(subset=['sexo'])\n",
    "\n",
    "# # Group by sex and calculate mean AP@7 and number of users\n",
    "# gender_summary = gender_df.groupby('sexo')['apk'].agg(['mean', 'count']).reset_index()\n",
    "# gender_summary.columns = ['sexo', 'mean_apk', 'n_users']\n",
    "\n",
    "# print(gender_summary)\n",
    "\n",
    "# # data visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.bar(gender_summary['sexo'].astype(str), gender_summary['mean_apk'])\n",
    "# plt.xlabel('Gender (0=Male, 1=Female)')\n",
    "# plt.ylabel('Mean AP@7')\n",
    "# plt.title('Recommendation Fairness by Gender')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeDo45i2coAu"
   },
   "source": [
    "## Fairness by Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "id": "awJ3h4bFBTPT",
    "outputId": "6e276c93-6488-437c-ec27-c2b0ec167353"
   },
   "outputs": [],
   "source": [
    "# # Drop rows with missing income\n",
    "# income_df = merged_df.dropna(subset=['renta'])\n",
    "\n",
    "# # Divide users into income groups based on tertiles\n",
    "# income_df['income_group'] = pd.qcut(income_df['renta'], 3, labels=['low', 'medium', 'high'])\n",
    "\n",
    "# # Group by income and calculate mean AP@7 and number of users\n",
    "# income_summary = income_df.groupby('income_group')['apk'].agg(['mean', 'count']).reset_index()\n",
    "# income_summary.columns = ['income_group', 'mean_apk', 'n_users']\n",
    "\n",
    "# print(income_summary)\n",
    "\n",
    "# # Visualization\n",
    "# plt.bar(income_summary['income_group'], income_summary['mean_apk'])\n",
    "# plt.xlabel('Income Group')\n",
    "# plt.ylabel('Mean AP@7')\n",
    "# plt.title('Recommendation Fairness by Income')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKrzOkk7B--Z"
   },
   "source": [
    "We evaluated fairness across gender and income groups based on AP@7. The average precision scores were [slightly higher / lower] for female users compared to male users, and [users in high-income groups received more accurate recommendations than those in low-income groups]. These differences may reflect behavioral patterns but also point to potential fairness concerns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATlc7h5-c1FO"
   },
   "source": [
    "## cross-group fairness analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "b9sChxtwdNqV"
   },
   "outputs": [],
   "source": [
    "# # Drop rows with missing renta or age\n",
    "# cross_df = merged_df.dropna(subset=['renta', 'age'])\n",
    "\n",
    "# # Categorize income\n",
    "# cross_df['income_group'] = pd.qcut(cross_df['renta'], 3, labels=['low', 'medium', 'high'])\n",
    "\n",
    "# # Categorize age\n",
    "# cross_df['age'] = cross_df['age'].astype(float)\n",
    "# cross_df['age_group'] = pd.cut(cross_df['age'],\n",
    "#                                bins=[0, 30, 45, 60, 75, 200],\n",
    "#                                labels=['18-30', '31-45', '46-60', '61-75', '76+'])\n",
    "\n",
    "# # Map gender\n",
    "# cross_df['sexo_label'] = cross_df['sexo'].map({0: 'man', 1: 'woman'})\n",
    "\n",
    "# # Combine all into a cross group\n",
    "# cross_df['group'] = cross_df['sexo_label'] + \"_\" + cross_df['income_group'].astype(str) + \"_\" + cross_df['age_group'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Rz-TP_QEriWo",
    "outputId": "d2b129f0-79be-4af7-eff9-7234e03b9748"
   },
   "outputs": [],
   "source": [
    "# cross_summary = cross_df.groupby('group')['apk'].agg(['mean', 'count']).reset_index()\n",
    "# cross_summary.columns = ['group', 'mean_apk', 'n_users']\n",
    "# cross_summary = cross_summary.sort_values(by='mean_apk', ascending=False)\n",
    "# print(cross_summary)\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.barplot(data=cross_summary, x='group', y='mean_apk')\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.xlabel('Gender + Income + Age Group')\n",
    "# plt.ylabel('Mean AP@7')\n",
    "# plt.title('Cross-Group Fairness: Mean AP@7 by Subgroup')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment - Save Trained Models\n",
    "\n",
    "This section saves all trained models and necessary artifacts for production deployment:\n",
    "- 24 LightGBM models (one per product)\n",
    "- Feature columns list (for consistent inference)\n",
    "- Product names list\n",
    "- Configuration and hyperparameters\n",
    "- Model performance metrics\n",
    "\n",
    "These files will be used by the FastAPI service to serve predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING MODEL EXPORT FOR DEPLOYMENT\n",
      "================================================================================\n",
      "\n",
      "[1/5] Saving LightGBM models...\n",
      "  ‚úì Saved: models/ind_ahor_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_cco_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_cder_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_cno_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_ctju_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_ctma_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_ctop_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_ctpp_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_dela_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_ecue_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_fond_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_hip_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_plan_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_pres_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_reca_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_tjcr_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_valo_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_viv_fin_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_nomina_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_nom_pens_ult1_model.txt\n",
      "  ‚úì Saved: models/ind_recibo_ult1_model.txt\n",
      "\n",
      "  ‚Üí Total models saved: 21 products\n",
      "\n",
      "[2/5] Saving feature columns list...\n",
      "  ‚úì Saved: models/feature_cols.json\n",
      "  ‚Üí Feature count: 193\n",
      "\n",
      "[3/5] Saving product list...\n",
      "  ‚úì Saved: models/products.json\n",
      "  ‚Üí Product count: 24\n",
      "\n",
      "[4/5] Saving configuration...\n",
      "  ‚úì Saved: models/config.json\n",
      "  ‚Üí Best alpha: 0.5\n",
      "  ‚Üí Validation MAP@7: 0.0153\n",
      "\n",
      "[5/5] Saving model metrics...\n",
      "  ‚úì Saved: models/metrics.json\n",
      "\n",
      "================================================================================\n",
      "‚úÖ MODEL EXPORT COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "All artifacts saved to 'models/' directory:\n",
      "  - 24 model files (*.txt)\n",
      "  - feature_cols.json (feature list)\n",
      "  - products.json (product list)\n",
      "  - config.json (configuration)\n",
      "  - metrics.json (performance metrics)\n",
      "\n",
      "================================================================================\n",
      "NEXT STEPS FOR DEPLOYMENT:\n",
      "================================================================================\n",
      "1. Verify models directory contains all files:\n",
      "   $ ls -lh models/\n",
      "\n",
      "2. Test API locally (development mode):\n",
      "   $ ./deploy.sh --local\n",
      "   or\n",
      "   $ uvicorn app:app --host 0.0.0.0 --port 8000 --reload\n",
      "\n",
      "3. Test API endpoints:\n",
      "   $ python predict_client.py\n",
      "\n",
      "4. Build Docker image (production deployment):\n",
      "   $ docker build -t santander-api .\n",
      "\n",
      "5. Run Docker container:\n",
      "   $ docker run -p 8000:8000 santander-api\n",
      "   or use automated script:\n",
      "   $ ./deploy.sh\n",
      "\n",
      "6. Access API documentation:\n",
      "   http://localhost:8000/docs\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# ========================================\n",
    "# STEP 1: Create Models Directory\n",
    "# ========================================\n",
    "# Create a directory to store all model artifacts\n",
    "# This directory will contain: model files, feature lists, configs, and metrics\n",
    "models_dir = 'models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STARTING MODEL EXPORT FOR DEPLOYMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ========================================\n",
    "# STEP 2: Save All Product Models\n",
    "# ========================================\n",
    "# Save each of the 24 LightGBM models (one per financial product)\n",
    "# Models are saved in LightGBM's native text format (.txt)\n",
    "# This format is efficient and compatible with the LightGBM API\n",
    "print(\"\\n[1/5] Saving LightGBM models...\")\n",
    "for product, model in models.items():\n",
    "    model_path = os.path.join(models_dir, f'{product}_model.txt')\n",
    "    model.save_model(model_path)\n",
    "    print(f\"  ‚úì Saved: {model_path}\")\n",
    "\n",
    "print(f\"\\n  ‚Üí Total models saved: {len(models)} products\")\n",
    "\n",
    "# ========================================\n",
    "# STEP 3: Save Feature Columns List\n",
    "# ========================================\n",
    "# Save the list of feature column names used during training\n",
    "# This ensures the API uses the same feature order during inference\n",
    "# Critical for model correctness!\n",
    "print(\"\\n[2/5] Saving feature columns list...\")\n",
    "feature_cols_path = os.path.join(models_dir, 'feature_cols.json')\n",
    "with open(feature_cols_path, 'w') as f:\n",
    "    json.dump(feature_cols, f, indent=2)\n",
    "print(f\"  ‚úì Saved: {feature_cols_path}\")\n",
    "print(f\"  ‚Üí Feature count: {len(feature_cols)}\")\n",
    "\n",
    "# ========================================\n",
    "# STEP 4: Save Product Names\n",
    "# ========================================\n",
    "# Save the list of all 24 product names\n",
    "# Used by the API to iterate over all products for recommendations\n",
    "print(\"\\n[3/5] Saving product list...\")\n",
    "products_path = os.path.join(models_dir, 'products.json')\n",
    "with open(products_path, 'w') as f:\n",
    "    json.dump(all_products, f, indent=2)\n",
    "print(f\"  ‚úì Saved: {products_path}\")\n",
    "print(f\"  ‚Üí Product count: {len(all_products)}\")\n",
    "\n",
    "# ========================================\n",
    "# STEP 5: Save Configuration\n",
    "# ========================================\n",
    "# Save model configuration and optimal hyperparameters\n",
    "# Includes: best alpha (for hybrid recommendation), MAP@7 score, metadata\n",
    "print(\"\\n[4/5] Saving configuration...\")\n",
    "config_path = os.path.join(models_dir, 'config.json')\n",
    "config = {\n",
    "    'best_alpha': best_alpha,          # Optimal blending weight for hybrid system\n",
    "    'best_map7': float(best_map),      # Best validation MAP@7 score\n",
    "    'num_products': len(all_products),  # Total number of products\n",
    "    'num_features': len(feature_cols),  # Total number of features\n",
    "    'recommendation_k': 7,              # Number of recommendations to return (top-7)\n",
    "    'model_type': 'LightGBM',          # Model algorithm\n",
    "    'training_date': '2016-04',        # Training data month\n",
    "    'validation_date': '2016-05',      # Validation data month\n",
    "    'description': 'Santander Product Recommendation System - Hybrid Model (LightGBM + Popularity Prior)'\n",
    "}\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"  ‚úì Saved: {config_path}\")\n",
    "print(f\"  ‚Üí Best alpha: {best_alpha}\")\n",
    "print(f\"  ‚Üí Validation MAP@7: {best_map:.4f}\")\n",
    "\n",
    "# ========================================\n",
    "# STEP 6: Save Model Performance Metrics\n",
    "# ========================================\n",
    "# Save individual model performance metrics (AUC for each product)\n",
    "# Useful for monitoring and debugging specific product predictions\n",
    "print(\"\\n[5/5] Saving model metrics...\")\n",
    "metrics_path = os.path.join(models_dir, 'metrics.json')\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(f\"  ‚úì Saved: {metrics_path}\")\n",
    "\n",
    "# ========================================\n",
    "# COMPLETION MESSAGE\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ MODEL EXPORT COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nAll artifacts saved to 'models/' directory:\")\n",
    "print(\"  - 24 model files (*.txt)\")\n",
    "print(\"  - feature_cols.json (feature list)\")\n",
    "print(\"  - products.json (product list)\")\n",
    "print(\"  - config.json (configuration)\")\n",
    "print(\"  - metrics.json (performance metrics)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NEXT STEPS FOR DEPLOYMENT:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. Verify models directory contains all files:\")\n",
    "print(\"   $ ls -lh models/\")\n",
    "print(\"\")\n",
    "print(\"2. Test API locally (development mode):\")\n",
    "print(\"   $ ./deploy.sh --local\")\n",
    "print(\"   or\")\n",
    "print(\"   $ uvicorn app:app --host 0.0.0.0 --port 8000 --reload\")\n",
    "print(\"\")\n",
    "print(\"3. Test API endpoints:\")\n",
    "print(\"   $ python predict_client.py\")\n",
    "print(\"\")\n",
    "print(\"4. Build Docker image (production deployment):\")\n",
    "print(\"   $ docker build -t santander-api .\")\n",
    "print(\"\")\n",
    "print(\"5. Run Docker container:\")\n",
    "print(\"   $ docker run -p 8000:8000 santander-api\")\n",
    "print(\"   or use automated script:\")\n",
    "print(\"   $ ./deploy.sh\")\n",
    "print(\"\")\n",
    "print(\"6. Access API documentation:\")\n",
    "print(\"   http://localhost:8000/docs\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf-macos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00e9b5299b8841f981878555e215d814": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "079f35dbf8ab45bf97662db069440cc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a8c8461a74b4176a4d07bc6bcc2d7c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39ccd7e70ef7437485eebae81000d41c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "456d58735cc243448e44af54dcf3f1a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_d4eca03a20164739abff98ac21a53ea8",
      "style": "IPY_MODEL_7e18d703ca454049a187049c1c8de759",
      "tooltip": ""
     }
    },
    "4b4b1be868bc481fb28722aa301af248": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "Username:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_534fed6904c74354ae37b0e9bc68bd0b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_fd894db53f514c87b9a6b4a10e93aca9",
      "value": "yiruchen123"
     }
    },
    "4cb8f4e109be48dbbe5dd7f62ad13bc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d0b174762b04c798cd899cb148bbd98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "534fed6904c74354ae37b0e9bc68bd0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "620112138d9b40a19cef8f7990b83228": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a8c8461a74b4176a4d07bc6bcc2d7c0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_079f35dbf8ab45bf97662db069440cc6",
      "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
     }
    },
    "642ba8ee50b04855ae74bffc09e87463": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65813552be444169b11461f772273422",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_7bd2188d711f4413aefa86eefa2273f2",
      "value": "Connecting..."
     }
    },
    "65813552be444169b11461f772273422": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bd2188d711f4413aefa86eefa2273f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7e18d703ca454049a187049c1c8de759": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "80224d2d21434971be0a3e8f57be05be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9016f44928844011ae026a58ea56aa24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a258e6d26384eff8779e40cdd7f6980": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5fe189267184be9bec0098740dfe462",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4cb8f4e109be48dbbe5dd7f62ad13bc4",
      "value": "Kaggle credentials successfully validated."
     }
    },
    "9a930b3294b945359a40aae3cc16defb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9016f44928844011ae026a58ea56aa24",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_00e9b5299b8841f981878555e215d814",
      "value": "\n<b>Thank You</b></center>"
     }
    },
    "a645f88718d542d493432b9f42b5f961": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9a258e6d26384eff8779e40cdd7f6980"
      ],
      "layout": "IPY_MODEL_39ccd7e70ef7437485eebae81000d41c"
     }
    },
    "b108174dd74446e18ad2dd39cccfa569": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_4d0b174762b04c798cd899cb148bbd98",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_80224d2d21434971be0a3e8f57be05be",
      "value": ""
     }
    },
    "b5fe189267184be9bec0098740dfe462": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4eca03a20164739abff98ac21a53ea8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd894db53f514c87b9a6b4a10e93aca9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
